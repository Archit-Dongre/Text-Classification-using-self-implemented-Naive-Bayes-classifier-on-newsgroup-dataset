{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a fit function by first taking all the files from the training data\n",
    "## Making stop words \n",
    "\n",
    "entries = os.listdir('Desktop/naivebayes/20_newsgroups')\n",
    "target_dict = {}\n",
    "for i in entries:\n",
    "    target_dict[i] = entries.index(i)\n",
    "f = open('Desktop/naivebayes/stopwords.txt')\n",
    "list_wrds = f.read()\n",
    "stop_words = list_wrds.split('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a feature dictionary here \n",
    "## choosing the first 5000 words for training purpose \n",
    "\n",
    "word_dict= {}\n",
    "w_dict = {}\n",
    "string_to_omit =\"! @ # $ % ^ & * ( ) 1 2 3 4 5 6 7 8 9 0 ' , { . < > /  = \\ + _ ? - | : ; + =\"\n",
    "string_to_omit_lis = string_to_omit.split()\n",
    "string_to_omit_lis = string_to_omit_lis + list(\"'\")+list('\"')\n",
    "for i in entries:\n",
    "    files_of_a_topic = os.listdir('Desktop/naivebayes/20_newsgroups/'+i)\n",
    "    word_dict = {}\n",
    "    for z in files_of_a_topic:\n",
    "        file = open('Desktop/naivebayes/20_newsgroups/'+i+'/'+z)\n",
    "        list_of_words_in_file = file.read().split()\n",
    "        for word_in_file in list_of_words_in_file:\n",
    "            if word_in_file in stop_words:\n",
    "                continue\n",
    "            elif word_in_file in string_to_omit:\n",
    "                continue\n",
    "            else:\n",
    "                if word_in_file in word_dict:\n",
    "                    word_dict[word_in_file] = word_dict[word_in_file]+1\n",
    "                else:\n",
    "                    p = 0 \n",
    "                    lis_to_check = list(word_in_file)\n",
    "                    for o in lis_to_check:\n",
    "                        if o in string_to_omit_lis :\n",
    "                            p = p-1\n",
    "                    if p==0:\n",
    "                        word_dict[word_in_file] =1\n",
    "    w_dict.update(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## chosing first 5000 words with most frequent occurences\n",
    "\n",
    "sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "lis = list(w_dict)\n",
    "\n",
    "features = list(lis[:5000])\n",
    "len(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the training data\n",
    "\n",
    "X = [[0]*5001]*19997\n",
    "X = np.array(X)\n",
    "row = 0 \n",
    "for class_val in entries:\n",
    "    files_of_a_topic = os.listdir('Desktop/naivebayes/20_newsgroups/'+class_val)\n",
    "    for doc in files_of_a_topic:\n",
    "        file = open('Desktop/naivebayes/20_newsgroups/'+class_val+'/'+doc)\n",
    "        list_of_words_in_file = file.read().split()\n",
    "        for word_in_file in list_of_words_in_file:\n",
    "            if word_in_file in features:\n",
    "                X[row][features.index(word_in_file)]  = X[row][features.index(word_in_file)] + 1\n",
    "        X[row][5000] = target_dict[class_val]\n",
    "        row = row + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the probability dictionary with each class as key \n",
    "## each class has another dictionary having the frequency of occurence of a particular feature for a given word in vocabulary \n",
    "   #for that particular doc of that class\n",
    "    \n",
    "probability_dict = {}\n",
    "for class_val in range(20):\n",
    "    probability_dict[class_val] = {}\n",
    "feature_dict = {}\n",
    "val = 0\n",
    "for i in features:\n",
    "    feature_dict[i] = val\n",
    "    val = val+1\n",
    "X_train = pd.DataFrame(X,columns=features+['cl'])\n",
    "\n",
    "## Making X train and Y Train \n",
    "\n",
    "classes = set(Y_train)\n",
    "for class_val in classes:\n",
    "    sum_of_all_points_in_a_class = 0\n",
    "    X_t = X_train[X_train.iloc[:,5000] == class_val]\n",
    "    b = probability_dict[class_val]\n",
    "    for feature in features:\n",
    "        freq = X_t.loc[:,feature].sum()\n",
    "        b[feature_dict[feature]] = freq\n",
    "        sum_of_all_points_in_a_class = sum_of_all_points_in_a_class + freq\n",
    "    b['total_points'] = sum_of_all_points_in_a_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_dict\n",
    "\n",
    "Y_t = X_train.loc[:,'cl']\n",
    "\n",
    "X_t = X_train.drop('cl',axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 18,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 14,\n",
       " 14,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 11,\n",
       " 19,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 12,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 18,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 12,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 12,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 12,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 13,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 5,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 13,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 12,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 11,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 14,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 11,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 13,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 16,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 18,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 16,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 12,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 19,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 16,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 14,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 18,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 19,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " ...]"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting output for out testing data\n",
    "\n",
    "def probability(list_of_words_in_file , class_val , proability_dict,features,Y_train):\n",
    "    denominator = probability_dict[class_val]['total_points']\n",
    "    x = 0 \n",
    "    for i in Y_t:\n",
    "        if i == class_val:\n",
    "            x+=1\n",
    "            \n",
    "    output = np.log(x) - np.log(len(Y_train))\n",
    "    ans = -11111111\n",
    "    for word in list_of_words_in_file:\n",
    "        if word not in features:\n",
    "            continue\n",
    "        else:\n",
    "            ##Applying Bayesian probability formula for logarithmic calculations and also implementing the laplacian correction\n",
    "            \n",
    "            current_word_probability = np.log(probability_dict[class_val][features.index(word)]+ 1) - np.log(denominator + 5000)\n",
    "            output = output + current_word_probability\n",
    "    ans = class_val\n",
    "    return output , ans\n",
    "            \n",
    "## predict function for a single point in test data \n",
    "\n",
    "def predict(file , probability_dict,features,Y_train):\n",
    "    list_of_words_in_file = file.read().split()\n",
    "    answer = -1000000\n",
    "    best_probability = -100000\n",
    "    run = True\n",
    "    for class_val in range(20):\n",
    "        probability_ans , ans = probability(list_of_words_in_file ,class_val,probability_dict,features,Y_train)\n",
    "        if(run or probability_ans>best_probability):\n",
    "            best_probability = probability_ans\n",
    "            answer = ans\n",
    "        run = False\n",
    "    return answer\n",
    "    \n",
    "\n",
    "    \n",
    "## going through test data to get predictions using predict function\n",
    "\n",
    "entries_test = os.listdir('Desktop/naivebayes/mini_newsgroups')\n",
    "string_to_omit_lis = string_to_omit_lis + list(\"'\")+list('\"')\n",
    "y_pred = []\n",
    "\n",
    "for i in entries_test:\n",
    "    files_of_a_topic_to_test = os.listdir('Desktop/naivebayes/mini_newsgroups/'+i)\n",
    "    for z in files_of_a_topic_to_test:\n",
    "        file = open('Desktop/naivebayes/mini_newsgroups/'+i+'/'+z)\n",
    "        y_pred.append(predict(file , probability_dict,features , Y_t))\n",
    "        \n",
    "\n",
    "y_pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7675"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## So accuracy of the self implemented model can now be calculated\n",
    "acc_lis = []\n",
    "for i in range(20):\n",
    "    acc_lis = acc_lis +  [i] *100\n",
    "    \n",
    "## acc_lis is the actual Y_values for test data\n",
    "    \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_pred,acc_lis)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy of self implemented model is 0.765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##implementing sklearn model for same data \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_t, Y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Making test data set\n",
    "\n",
    "X_test = [[0]*5000]*2000\n",
    "X_test = np.array(X_test)\n",
    "row = 0 \n",
    "for class_val in entries_test:\n",
    "    files_of_a_topic = os.listdir('Desktop/naivebayes/mini_newsgroups/'+class_val)\n",
    "    for doc in files_of_a_topic:\n",
    "        file = open('Desktop/naivebayes/mini_newsgroups/'+class_val+'/'+doc)\n",
    "        list_of_words_in_file = file.read().split()\n",
    "        for word_in_file in list_of_words_in_file:\n",
    "            if word_in_file in features:\n",
    "                X_test[row][features.index(word_in_file)]  = X_test[row][features.index(word_in_file)] + 1\n",
    "        row = row + 1\n",
    "X_test = pd.DataFrame(X_test,columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([18,  0,  0, ..., 19, 19, 19])"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using predict function \n",
    "\n",
    "Y_test_pred  = clf.predict(X_test)\n",
    "\n",
    "ac = accuracy_score(acc_lis,Y_test_pred)\n",
    "print(ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Accuracy of sklearn model is 0.765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
